# Awsome-Time-Series-Foundation-Model

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re) [![MIT License](https://img.shields.io/badge/license-MIT-green.svg)](https://opensource.org/licenses/MIT) [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)

<p align="center">
  <img width="300" src="https://i.imgur.com/Ky2jxnj.png" "Awesome!">
</p>

An up-to-date & curated list of Awsome Time Series Foundation Model papers, methods & resources.

## Acknowledgment

Many thanks to the useful publications and repos: [Jingdong Wang](https://github.com/jindongwang), [Awesome-Deep-Vision](https://github.com/kjw0612/awesome-deep-vision), [Awesome-Deep-Learning-Papers](https://github.com/terryum/awesome-deep-learning-papers), [Awesome-Self-Supervised-Learning](https://github.com/jason718/awesome-self-supervised-learning), [Awesome-Semi-Supervised-Learning](https://github.com/yassouali/awesome-semi-supervised-learning), [Awesome-Human-Activity-Recognition
](https://github.com/haoranD/Awesome-Human-Activity-Recognition/) and [Awesome-Crowd-Counting](https://github.com/gjy3035/Awesome-Crowd-Counting#datasets).

## Contributing
<p align="center">
  <img src="http://cdn1.sportngin.com/attachments/news_article/7269/5172/needyou_small.jpg" alt="We Need You!">
</p>

Please feel free to contribute this list.


## Conferences, Journals and Workshops
- IJCAI, ACM MultiMedia, AAAI, KDD, ICDM, TKDE, TIP, TNNLS, TPAMI, TMM, Pattern Recognition, AI, Nature Communication, Nature Digital Medicine, ICPR, Sensors, Ubicomp(IMWUT Journal)

## Datasets

- Capture-24 [[**link**](https://github.com/OxWearables/capture24)]
- [dataset.pdf](https://github.com/user-attachments/files/18184364/dataset.pdf)
| Field 	| Dataset Name 	| Types of signal 	| SNR levels 	| Data volume 	| Illustrate 	|
|---	|---	|---	|---	|---	|---	|
| Communication Field 	| RML 2016.10a 	| 8 digital modulations, 3 analog modulations 	| [-20,+18] 	| 220000 	| Modulated Signal 	|
|  	| RML 2016.10b 	| 8 digital modulations, 2 analog modulations 	| [-20,+18] 	| 1200000 	| Modulated Signal 	|
|  	| RML 2016.04c 	| 8 digital modulations, 3 analog modulations 	| [-20,+18] 	| 162060 	| Modulated Signal 	|
|  	| RML 2018.01a 	| 24 modulation methods 	| [-20,+18] 	| 2555904 	| Modulated Signal 	|
|  	| HisarMod 2019.1 	| 26 modulation methods 	| [-20,+18] 	| 78000 	| Modulated Signal 	|
|  	| Sig53 	| 53 modulation methods 	| [-20,+30] 	| 6 million 	| Modulated Signal 	|
|  	| Communication-Signal-Dataset 	| 11 modulation methods 	| [0,+20] 	| 22000 	| Modulated Signal 	|
|  	| AugMod 	| 7 modulation methods 	| [0,+40] 	| 174720 	| Modulated Signal 	|
|  	| Cellular communication Signal dataset 	| Cellular communication signals in SCF format 	| [+1,+15] 	| 60000 	| / 	|
|  	| DeepMIMO 	| Wireless signal 	|  	|  	| Outdoor ray tracing scenes with 18 base stations and over 1 million users 	|
|  	| Mallat 	| Analog signals 	| / 	| Train size:55;Test size:2345:Length:1024;Classes:8;Dimensions:1 	|  	|
|  	| ShapeletSim 	| Analog signals 	| / 	| Train size:20;Test size:180:Length:500;Classes:2;Dimensions:1 	|  	|
|  	|  	|  	|  	|  	|  	|
| Biomedical Field 	| PhyAAT 	| EEG(14 channels),GSR（2）,PPG（3） 	| / 	|  	|  	|
|  	| ECG Heartbeat Categorization Dataset 	| ECG 	| / 	| 109446,14552 	| The MIT-BIH Arrhythmia Dataset and The PTB Diagnostic ECG Database. 	|
|  	| The Apnea-ECG database 	| ECG 	| / 	| It contains 70 Electrocardiography records 	|  	|
|  	| SEED 	| ECG 	| / 	| Physiological data from 32 participants. 	| The physiological data consists of 40 features — 32 channels of EEG readings; and a further 8 peripheral readings 	|
|  	| AtrialFibrillation 	| ECG 	| / 	| 5 second segments of atrial fibrillation, containing two ECG signals 	| each sampled at 128 samples per second. The class labels are: n, s and t.<br>Train size:15;Test size:15 	|
|  	| Blink 	| EEG 	| / 	| Collected in 20 trials of six patients, with 50 blinks per experiment. 	| Each sample is 510 values long, with 4 EEG channels, representing 2 seconds sampled at 255Hz.<br>Train size:500;Test size:450 	|
|  	| CardiacArrhythmia 	| ECG 	| / 	|  	|  	|
|  	| ECG200 	| ECG 	| / 	| Train size:100;Test size:100:Length:96;Classes:2;Dimensions:1 	|  	|
|  	| ECG5000 	| ECG 	| / 	| Train size:500;Test size:4500:Length:140;Classes:5;Dimensions:1 	|  	|
|  	| ECGFiveDays 	| ECG 	| / 	| Train size:23;Test size:861:Length:136;Classes:2;Dimensions:1 	|  	|
|  	| EOGHorizontalSignal 	| EOG 	| / 	| Train size:362;Test size:362:Length:1250;Classes:12;Dimensions:1 	|  	|
|  	| EOGVerticalSignal 	| EOG 	| / 	| Train size:362;Test size:362:Length:1250;Classes:12;Dimensions:1 	|  	|
|  	| Epilepsy2 	| EEG 	| / 	| Train size:80;Test size:11420:Length:178;Classes:2;Dimensions:1 	|  	|
|  	| EyesOpenShut 	| EEG 	| / 	| Train size:56;Test size:42:Length:128;Classes:2;Dimensions:14   	|  	|
|  	| FaceDetection 	| EEG 	| / 	| Train size:5890;Test size:3524:Length:62;Classes:2;Dimensions:144   	|  	|
|  	| FingerMovements 	| EEG 	| / 	| Train size:316;Test size:100:Length:50;Classes:2;Dimensions:28   	|  	|
|  	| HandMovementDirection 	| EEG 	| / 	| Train size:160;Test size:74:Length:400;Classes:4;Dimensions:10   	|  	|
|  	| MindReading 	| MEG 	| / 	| Train size:727;Test size:653:Length:200;Classes:5;Dimensions:204   	|  	|
|  	| MotorImagery 	| EEG 	| / 	| Train size:278;Test size:100:Length:3000;Classes:2;Dimensions:64   	|  	|
|  	| NerveDamage 	| EMG 	| / 	| Train size:163;Test size:41:Length:1500;Classes:3;Dimensions:1   	|  	|
|  	| NonInvasiveFetalECGThorax1 	| ECG 	| / 	| Train size:1800;Test size:1965:Length:750;Classes:42;Dimensions:1   	|  	|
|  	| NonInvasiveFetalECGThorax2 	| ECG 	| / 	| Train size:1800;Test size:1965:Length:750;Classes:42;Dimensions:1   	|  	|
|  	| SelfRegulationSCP1 	| EEG 	| / 	| Train size:268;Test size:293:Length:896;Classes:2;Dimensions:6  	|  	|
|  	| SelfRegulationSCP2 	| EEG 	| / 	| Train size:200;Test size:180:Length:1152;Classes:2;Dimensions:7  	|  	|
|  	| Sleep 	| EEG 	| / 	| Train size:478785;Test size:90315:Length:178;Classes:5;Dimensions:1 	|  	|
|  	| StandWalkJump 	| ECG 	| / 	| Train size:12;Test size:15:Length:2500;Classes:3;Dimensions:4 	|  	|
|  	|  	|  	|  	|  	|  	|
| Meteorology and environment Field 	| Earthquake dataset 	| Earthquake data 	| / 	|  	| contain records of 782 earthquakes from 1/1/2001 to 1/1/2023 	|
|  	|  	|  	|  	|  	|  	|
| Transportation Field 	| DodgerLoopDay 	| traffic data 	| / 	| Train size:78;Test size:80:Length:288;Classes:7;Dimensions:1 	|  	|
|  	| DodgerLoopGame 	| traffic data 	| / 	| Train size:20;Test size:138:Length:288;Classes:2;Dimensions:1 	|  	|
|  	| MelbournePedestrian 	| traffic data 	| / 	| Train size:1194;Test size:2439:Length:24;Classes:10;Dimensions:1 	|  	|
|  	|  	|  	|  	|  	|  	|
| Industrial Field 	| MoteStrain 	| Sensor data 	| / 	| Train size:20;Test size:1252:Length:84;Classes:2;Dimensions:1 	|  	|
|  	| PLAID 	| current and voltage measurements 	| / 	| Train size:537;Test size:537;Classes:11;Dimensions:1 	|  	|
|  	| PowerCons 	| electric power consumption 	| / 	| Train size:180;Test size:180:Length:144;Classes:2;Dimensions:1 	|  	|
|  	| SonyAIBORobotSurface1 	| Sensor data 	| / 	| Train size:20;Test size:601:Length:70;Classes:2;Dimensions:1 	|  	|
|  	| SonyAIBORobotSurface2 	| Sensor data 	| / 	| Train size:27;Test size:953:Length:65;Classes:2;Dimensions:1 	|  	|
|  	| Wafer 	| Sensor data 	| / 	| Train size:1000;Test size:6164:Length:152;Classes:2;Dimensions:1 	|  	|
|  	|  	|  	|  	|  	|  	|
| Radar Field 	|  	|  	|  	|  	|  	|

## Tools

## Other related tasks

## Potential Research Direction

## Papers

### Surveys & Overview

- <a name="TSPTM"></a> **[TSPTM]** A Survey on Time-Series Pre-Trained Models: A Survey (**IEEE Transactions on Knowledge and Data Engineering**) [[paper](https://arxiv.org/abs/2305.10716)][[code](https://github.com/qianlima-lab/time-series-ptms)]

### 2024

- <a name="Time-LLM"></a> **[Time-LLM]** Time-LLM: Time series forecasting by reprogramming large language models: (**ICLR 2024**)[[paper](https://arxiv.org/pdf/2310.01728)][[code](https://github.com/KimMeen/Time-LLM)]

- <a name="MTP-Flow"></a> **[MTP-Flow]** Masked token enabled pre-training: A task-agnostic approach for understanding complex traffic flow: (**IEEE Transactions on Mobile Computing**)[[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10505023)][[code](https://github.com/Xiao-Di/TSSN)]

- <a name="TEMPO"></a> **[TEMPO]** TEMPO: Prompt-based generative pre-trained transformer for time series forecasting: (**ICLR 2024**)[[paper](https://arxiv.org/pdf/2310.04948)][[code](https://github.com/DC-research/TEMPO)]

- <a name="iTransformer"></a> **[iTransformer]** Inverted transformers are effective for time series forecasting: (**ICLR 2024**)[[paper](https://arxiv.org/pdf/2310.06625)][[code](https://github.com/thuml/iTransformer)]

### 2023

- <a name="PatchTST"></a> **[PatchTST]** A time series is worth 64 words: Long-term forecasting with transformers: (**ICLR 2023**)[[paper](https://arxiv.org/pdf/2211.14730)][[code](https://github.com/PatchTST/PatchTST)]

- <a name="TimesNet"></a> **[TimesNet]** TimesNet: Temporal 2D-variation modeling for general time series analysis: (**ICLR 2023**)[[paper](https://arxiv.org/pdf/2210.02186)][[code](https://github.com/thuml/TimesNet)]

- <a name="OFa"></a> **[OFa]** One fits all: Power general time series analysis by pretrained LM: (**NeurIPS 2023**)[[paper](https://arxiv.org/pdf/2302.11939)][[code](https://github.com/DAMO-DI-ML/NeurIPS2023-One-Fits-All)]

- <a name="D-Linear"></a> **[D-Linear]** Are transformers effective for time series forecasting?: (**AAAI 2023**)[[paper](https://arxiv.org/pdf/2205.13504)][[code](https://github.com/cure-lab/LTSF-Linear)]

### 2022

- <a name="FEDformer"></a>  **[FEDformer]** FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting: (**IEEE International Conference on Data Mining**)[[paper](https://arxiv.org/pdf/2201.12740)][[code](https://github.com/MAZiqing/FEDformer)]

- <a name="CR-Transformer"></a>  **[CR-Transformer]** Cross reconstruction transformer for self-supervised time series representation learning: (**ICML 2021**)[[paper](https://arxiv.org/pdf/2205.09928)][[code](https://github.com/BobZwr/Cross-Reconstruction-Transformer)]

- <a name="TS2Vec"></a>  **[TS2Vec]** TS2Vec: Towards universal representation of time series: (**ICDM 2021**)[[paper](https://arxiv.org/pdf/2106.10466v2)][[code](https://github.com/zhihanyue/ts2vec)]

- <a name="CoST"></a>  **[CoST]** CoST: Contrastive learning of disentangled seasonal-trend representations for time series forecasting: (**ICLR 2022**)[[paper](https://arxiv.org/pdf/2202.01575)][[code](https://github.com/salesforce/CoST)]

### 2021

- <a name="TSDA-SASA"></a>  **[TSDA-SASA]** Time series domain adaptation via sparse associative structure alignment: (**AAAI Conf. Artif. Intell**)[[Paper](https://arxiv.org/abs/2012.11797)] [[Code](https://github.com/DMIRLAB-Group/SASA-pytorch)]

- <a name="Autoformers"></a>  **[Autoformer]** Autoformer: Decomposition transformers with autocorrelation for long-term series forecasting: (**NeurIPS 2021**)[[Paper](https://arxiv.org/pdf/2106.13008)][[Code](https://github.com/thuml/Autoformer)]

- <a name="Transformers4MTS"></a>  **[Transformers4MTS]** A transformer-based framework for multivariate time series representation learning: (**KDD 2021**)[[Paper](https://arxiv.org/pdf/2010.02803)][[Code](https://github.com/gzerveas/mvts_transformer)]

- <a name="TabFormer"></a>  **[TabFormer]** Tabular transformers for modeling multivariate time series: (**ICLR 2021**)[[Paper](https://arxiv.org/pdf/2011.01843v1)][[Code](https://github.com/IBM/TabFormer)]

- <a name="Informer"></a>  **[Informer]** Informer: Beyond efficient transformer for long sequence time-series forecasting: (**AAAI 2021**)[[Paper](https://arxiv.org/pdf/2012.07436)][[Code](https://github.com/zhouhaoyi/Informer2020)]

### 2020

- <a name="Self-Time"></a>  **[Self-Time]** Self-supervised time series representation
learning by inter-intra relational reasoning: (**Under Review**)[[Paper](https://arxiv.org/pdf/2011.13548)][[Code](https://github.com/haoyfan/SelfTime)]

### 2019

- <a name="BERT"></a>  **[BERT]** BERT: Pre-training of deep bidirectional transformers for language understanding:(**NAACL-HLT 2019**)[[Paper](https://arxiv.org/pdf/1810.04805)][[Code](https://github.com/google-research/bert)]

- <a name="USRL-MTS"></a>  **[USRL-MTS]** Unsupervised scalable representation learning for multivariate time series: (**NeurIPS 2019**)[[Paper](https://arxiv.org/pdf/1901.10738)][[Code](https://github.com/White-Link/UnsupervisedScalableRepresentationLearningTimeSeries)]

- <a name="ELB-MB-Transformer"></a>  **[ELB-MB-Transformer]** Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting: (**NeurIPS 2019**)[[Paper](https://arxiv.org/pdf/1907.00235)][[Code](https://github.com/mlpotter/Transformer_Time_Series)]

### 2018

- <a name="TL4TSC"></a> **[TL4TSC]** Transfer Learning for Time Series Classification: (**IEEE Big Data 2018**) [[paper](https://arxiv.org/pdf/1811.01533)][[code](https://github.com/hfawaz/bigdata18?tab=readme-ov-file)]

