# Awsome-Time-Series-Foundation-Model

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re) [![MIT License](https://img.shields.io/badge/license-MIT-green.svg)](https://opensource.org/licenses/MIT) [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)

<p align="center">
  <img width="300" src="https://i.imgur.com/Ky2jxnj.png" "Awesome!">
</p>

An up-to-date & curated list of Awsome Time Series Foundation Model papers, methods & resources.

## Acknowledgment

Many thanks to the useful publications and repos: [Jingdong Wang](https://github.com/jindongwang), [Awesome-Deep-Vision](https://github.com/kjw0612/awesome-deep-vision), [Awesome-Deep-Learning-Papers](https://github.com/terryum/awesome-deep-learning-papers), [Awesome-Self-Supervised-Learning](https://github.com/jason718/awesome-self-supervised-learning), [Awesome-Semi-Supervised-Learning](https://github.com/yassouali/awesome-semi-supervised-learning), [Awesome-Human-Activity-Recognition
](https://github.com/haoranD/Awesome-Human-Activity-Recognition/) and [Awesome-Crowd-Counting](https://github.com/gjy3035/Awesome-Crowd-Counting#datasets).

## Contributing
<p align="center">
  <img src="http://cdn1.sportngin.com/attachments/news_article/7269/5172/needyou_small.jpg" alt="We Need You!">
</p>

Please feel free to contribute this list.


## Conferences, Journals and Workshops
- IJCAI, ACM MultiMedia, AAAI, KDD, ICDM, TKDE, TIP, TNNLS, TPAMI, TMM, Pattern Recognition, AI, Nature Communication, Nature Digital Medicine, ICPR, Sensors, Ubicomp(IMWUT Journal)

## Datasets

- Capture-24 [[**link**](https://github.com/OxWearables/capture24)]

## Tools

## Other related tasks

## Potential Research Direction

## Papers

### Surveys & Overview

- <a name="DL4SAR"></a> **[DL4SAR]** Deep Learning for Sensor-based Activity Recognition: A Survey (**Pattern Recognition Letters**) [[paper](https://arxiv.org/pdf/1707.03502.pdf)][[code](https://github.com/jindongwang)]

### 2024

- <a name="DL4SAR"></a> **[DL4SAR]** Deep Learning for Sensor-based Activity Recognition: A Survey (**Pattern Recognition Letters**) [[paper](https://arxiv.org/pdf/1707.03502.pdf)][[code](https://github.com/jindongwang)]

- <a name="TSPTM"></a> **[TSPTM]** A Survey on Time-Series Pre-Trained Models: A Survey (**IEEE Transactions on Knowledge and Data Engineering**) [[paper](https://arxiv.org/abs/2305.10716)][[code](https://github.com/qianlima-lab/time-series-ptms)]

- <a name="Time-LLM"></a> **[Time-LLM]** Time-LLM: Time series forecasting by reprogramming
large language models: (**Proc. 12th Int. Conf. Learn. Representations**)[[paper](https://arxiv.org/pdf/2310.01728)][[code](https://github.com/KimMeen/Time-LLM)]

### 2022

- <a name="FEDformer"></a>  **[FEDformer]** FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting: (**IEEE International Conference on Data Mining**)[[paper](https://arxiv.org/pdf/2201.12740)][[code](https://github.com/MAZiqing/FEDformer)]

### 2021

- <a name="TSDA-SASA"></a>  **[TSDA-SASA]** Time series domain adaptation via sparse associative structure alignment: (**Proc. AAAI Conf. Artif. Intell**)[[Paper](https://arxiv.org/abs/2012.11797)] [[Code](https://github.com/DMIRLAB-Group/SASA-pytorch)]

- <a name="Voice2Series"></a>  **[Voice2Series]** Voice2Series: Reprogramming acoustic models for time series classification: (**Proc. 38th Int. Conf. Mach. Learn.**)[[Paper]([https://arxiv.org/pdf/2106.09296])][[Code](https://github.com/huckiyang/Voice2Series-Reprogramming)]

- <a name="Autoformers"></a>  **[Autoformer]** Autoformer: Decomposition transformers with autocorrelation for long-term series forecasting: (**Proc. Adv. Neural Inf. Process. Syst.**)[[Paper]([https://arxiv.org/pdf/2106.13008])][[Code](https://github.com/thuml/Autoformer)]

### 2020

- <a name="MSDDA-WS"></a>  **[MSDDA-WS]** Multi-source deep domain adaptation with weak supervision for time-series sensor data: (**Proc. 26th ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining**)[[Paper](https://arxiv.org/pdf/2005.10996)][[Code](https://github.com/floft/codats)]

### 2019

- <a name="vq-wav2vec"></a>  **[vq-wav2vec]** vq-wav2vec: Self-supervised learning of discrete speech representations: (**34th Conference on Neural Information Processing Systems**)[[Paper](https://arxiv.org/pdf/1910.05453)][[Code](https://github.com/facebookresearch/fairseq)]

### 2018

- <a name="TL4TSC"></a> **[TL4TSC]** Transfer Learning for Time Series Classification: (**2018 IEEE International Conference on Big Data**) [[paper](https://arxiv.org/pdf/1811.01533)][[code](https://github.com/hfawaz/bigdata18?tab=readme-ov-file)]


### 2016

- <a name="Audio Word2Vec"></a> **[Audio Word2Vec]** Audio Word2Vec: Unsupervised learning of audio segment representations using sequence-to-sequence autoencoder: (**IEEE International Conference on Acoustics, Speech, and Signal Processing**)[[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8462332)][[code](https://github.com/jcvasquezc/DisVoice)]


### 2014

- <a name="Seq2Seq"></a> **[Seq2Seq]** Sequence to sequence learning with neural networks: (**Proc. Adv. Neural Inf. Process. Syst.**) [[paper](https://arxiv.org/pdf/1409.3215)][[code](https://github.com/farizrahman4u/seq2seq?tab=readme-ov-file)]


